{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-1. Daum 뉴스기사 제목 스크래핑하기 \n",
    " #질문1 :  아래의 url에서 뉴스기사의 링크와 제목을 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5407f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. URL 설정\n",
    "url = 'https://news.daum.net/economy'\n",
    "\n",
    "# 2. 요청 보내기 (User-Agent 설정 권장)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "\n",
    "# 3. 인코딩 설정\n",
    "# 다음 뉴스 서버에서 보내주는 인코딩 방식과 파이썬이 해석하는 방식이 다를 경우\n",
    "# 한글이 깨질 수 있어 이를 방지하기 위해 강제로 utf-8로 설정했습니다.\n",
    "res.encoding = 'utf-8'\n",
    "\n",
    "# 4. 응답 확인 및 Soup 객체 생성\n",
    "if res.ok:\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    # 5. 뉴스 리스트 가져오기 (CSS 선택자 사용)\n",
    "    news_list = soup.select('ul.list_newsheadline2 li')\n",
    "    \n",
    "    print(f'검색된 기사 수: {len(news_list)}개')\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 6. 리스트 순회하며 제목과 링크 추출\n",
    "    for li_tag in news_list:\n",
    "        # (1) 링크(href) 추출 - find 사용\n",
    "        a_tag = li_tag.find('a')\n",
    "        if a_tag: # a 태그가 존재하는 경우에만 실행\n",
    "            link = a_tag['href']\n",
    "            \n",
    "            # (2) 제목(text) 추출 - select_one 사용\n",
    "    \n",
    "\n",
    "            strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "            \n",
    "            # strong_tag가 존재하는지 확인 후 텍스트 추출\n",
    "            if strong_tag:\n",
    "                title = strong_tag.text.strip() # 공백 제거\n",
    "                print(f'제목: {title}')\n",
    "                print(f'링크: {link}')\n",
    "                print() # 줄바꿈\n",
    "            else:\n",
    "                # 만약 cont_thumb 구조가 아니라면 바로 a 태그의 텍스트일 수도 있습니다.\n",
    "                print(f'제목: {a_tag.text.strip()}')\n",
    "                print(f'링크: {link}')\n",
    "                print()\n",
    "\n",
    "else:\n",
    "    print(f'Error 발생: {res.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#질문2:  여러개의 section 중 하나를 선택해서 url에서 뉴스기사의 링크와 제목을 출력하는 코드를 함수로 작성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b10465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. 뉴스 분야 URL 식별자 사전 \n",
    "section_dict = {\n",
    "     '경제':'economy'\n",
    "   \n",
    "}\n",
    "\n",
    "# 2. 함수 선언\n",
    "def print_news(section_name):\n",
    "  \n",
    "   \n",
    "    section_code = section_dict.get(section_name)\n",
    "    \n",
    "    # 사전에 없는 분야가 입력되었을 경우 처리\n",
    "    if not section_code:\n",
    "        print(f\"'{section_name}'는 지원하지 않는 분야입니다.\")\n",
    "        return\n",
    "\n",
    "    # (2) URL 생성\n",
    "    url = f'https://news.daum.net/{section_code}'\n",
    "    print(f\"== {section_name} 뉴스 ({url}) ==\")\n",
    "\n",
    "    # 요청 헤더 및 요청 보내기\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding = 'utf-8' # 다음 뉴스는 인코딩 설정 \n",
    "\n",
    "    # (4) 데이터 파싱 및 추출 (1번 문제 로직 동일)\n",
    "    if res.ok:\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        # 기사 목록 가져오기\n",
    "        news_list = soup.select('ul.list_newsheadline2 li')\n",
    "        \n",
    "        count = 0\n",
    "        for li_tag in news_list:\n",
    "            a_tag = li_tag.find('a')\n",
    "            if a_tag:\n",
    "                title = a_tag.text.strip() # 제목에서 공백 제거\n",
    "                link = a_tag['href']\n",
    "                \n",
    "                # 제목이 비어있는 경우(이미지 태그 등) 대비하여 \n",
    "                # tit_txt 클래스를 한 번 더 찾습니다.\n",
    "                if not title:\n",
    "                     strong = li_tag.select_one('.tit_txt')\n",
    "                     if strong:\n",
    "                         title = strong.text.strip()\n",
    "\n",
    "                print(f\"{count+1}. {title}\")\n",
    "                print(f\"   링크: {link}\")\n",
    "                count += 1\n",
    "                \n",
    "                #  5개만 출력\n",
    "                if count >= 5: \n",
    "                    break \n",
    "        print(\"-\" * 50) # 구분선\n",
    "    else:\n",
    "        print(f\"통신 에러 발생: {res.status_code}\")\n",
    "\n",
    "# 함수 호출 테스트\n",
    "print_news('경제')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
